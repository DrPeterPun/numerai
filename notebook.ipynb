{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['v4.1/features.json',\n",
       " 'v4.1/live.parquet',\n",
       " 'v4.1/live_example_preds.csv',\n",
       " 'v4.1/live_example_preds.parquet',\n",
       " 'v4.1/live_int8.parquet',\n",
       " 'v4.1/meta_model.parquet',\n",
       " 'v4.1/train.parquet',\n",
       " 'v4.1/train_int8.parquet',\n",
       " 'v4.1/validation.parquet',\n",
       " 'v4.1/validation_example_preds.csv',\n",
       " 'v4.1/validation_example_preds.parquet',\n",
       " 'v4.1/validation_int8.parquet']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize NumerAPI - the official Python API client for Numerai\n",
    "from numerapi import NumerAPI\n",
    "napi = NumerAPI()\n",
    "\n",
    "# Print all files available for download in the latest v4.1 dataset\n",
    "[f for f in napi.list_datasets() if f.startswith(\"v4.1\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-22 19:23:47,224 INFO numerapi.utils: resuming download\n",
      "/home/peter/Documents/numerai/lib/python3.11/site-packages/urllib3/connectionpool.py:1095: InsecureRequestWarning: Unverified HTTPS request is being made to host 'numerai-datasets-us-west-2.s3.amazonaws.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "v4.1/train.parquet:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 1.34G/1.45G [21:25:10<11:58, 163kB/s]  "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Download the training data and feature metadata\n",
    "# This may take a few minutes üçµ\n",
    "napi.download_dataset(\"v4.1/train.parquet\");\n",
    "napi.download_dataset(\"v4.1/features.json\");\n",
    "\n",
    "# Load the training data but only the \"small\" subset of features to save time and memory\n",
    "# In practice you will want to use all the features to maximize your model's performance\n",
    "feature_metadata = json.load(open(\"v4.1/features.json\")) \n",
    "feature_cols = feature_metadata[\"feature_sets\"][\"small\"]\n",
    "training_data = pd.read_parquet(\"v4.1/train.parquet\", columns= [\"era\"] + feature_cols + [\"target\"]) \n",
    "\n",
    "# Print the training data\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', 5)\n",
    "training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "o objetivo aqui √© criar um modelo para submeter no numer.ai\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM\n",
    "isto faz trees\n",
    "primeiro vou criar um modelo com as opcoes default dadas pelo numerai para ver como se sai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "try:\n",
    "    # load aos modelos de treino\n",
    "    model = lgb.Booster(model_file='small_lgb.model');\n",
    "except lgb.basic.LightGBMError:\n",
    "    # criar o modelo\n",
    "    model = lgb.LGMRegressor(\n",
    "        n_estimators = 2000,\n",
    "        learning_rate = 0.01,\n",
    "        max_depth = 5,\n",
    "        num_leaves = 2**5,\n",
    "        colsample_bytree = 0.1\n",
    "    )\n",
    "\n",
    "    #treinar o modelo\n",
    "    model.fit(\n",
    "        training_data[feature_cols],\n",
    "        training_data[\"target\"]\n",
    "    )\n",
    "\n",
    "    # gravar o modelo\n",
    "    model.booster_.save_model(\"small_lbg_tree.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testar o modelo agora\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dl do dataset de validation\n",
    "napi.download_dataset(\"v4.1/validation.parquet\")\n",
    "validation_data = pd.read_parquet(\"v4.1/validation.parquet\", columns=[\"era\", \"data_type\"] + feature_cols + [\"target\"]) \n",
    "\n",
    "# load do dataset de treino\n",
    "validation_data = validation_data[validation_data[\"data_type\"] == \"validation\" ]\n",
    "del validation_data[\"data_type\"]\n",
    "\n",
    "# gerar as previsoes\n",
    "validation_data[\"prediction\"] = model.predict(validation_data[feature_cols])\n",
    "validation_data[[\"era\", \"prediction\", \"target\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definir a funcao de scoring para corr\n",
    "esta √© a maneira principal de scoring do numerai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "# Numerai's primary scoring metric\n",
    "def numerai_corr(preds, target):\n",
    "    # rank (keeping ties) then gaussianize predictions to standardize prediction distributions\n",
    "    ranked_preds = (preds.rank(method=\"average\").values - 0.5) / preds.count()\n",
    "    gauss_ranked_preds = stats.norm.ppf(ranked_preds)\n",
    "    # center targets around 0\n",
    "    centered_target = target - target.mean()\n",
    "    # raise both preds and target to the power of 1.5 to accentuate the tails\n",
    "    preds_p15 = np.sign(gauss_ranked_preds) * np.abs(gauss_ranked_preds) ** 1.5\n",
    "    target_p15 = np.sign(centered_target) * np.abs(centered_target) ** 1.5\n",
    "    # finally return the Pearson correlation\n",
    "    return np.corrcoef(preds_p15, target_p15)[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlacao /era das previsoes com o metamodelo + um plot\n",
    "\n",
    "per_era_corr = validation_data.groupby(\"era\").apply(lambda x: numerai_corr(x[\"prediction\"], x[\"target\"]))\n",
    "per_era_corr.plot(kind=\"bar\", title=\"Validation Correlation\", figsize=(10, 6), xticks=[]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot da corr/cummulatia / era, mais util porque permite ter nocao de \"quanto ganha\"__qualname__ \n",
    "per_era_corr.cumsum().plot(kind=\"line\", title=\"Cumulative Validation Correlation\", figsize=(10, 6));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "mesma coisa mas com xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# Initialize the XGBoost regression model\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(\n",
    "    training_data[feature_cols],\n",
    "    training_data[\"target\"]\n",
    "    )\n",
    "\n",
    "# dl do dataset de validation\n",
    "napi.download_dataset(\"v4.1/validation.parquet\")\n",
    "validation_data = pd.read_parquet(\"v4.1/validation.parquet\", columns=[\"era\", \"data_type\"] + feature_cols + [\"target\"]) \n",
    "\n",
    "# load do dataset de treino\n",
    "validation_data = validation_data[validation_data[\"data_type\"] == \"validation\" ]\n",
    "del validation_data[\"data_type\"]\n",
    "\n",
    "# gerar as previsoes\n",
    "validation_data[\"prediction\"] = model.predict(validation_data[feature_cols])\n",
    "validation_data[[\"era\", \"prediction\", \"target\"]]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "numerai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
