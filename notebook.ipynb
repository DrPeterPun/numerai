{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize NumerAPI - the official Python API client for Numerai\n",
    "from numerapi import NumerAPI\n",
    "napi = NumerAPI()\n",
    "\n",
    "# Print all files available for download in the latest v4.1 dataset\n",
    "[f for f in napi.list_datasets() if f.startswith(\"v4.2\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numerapi import NumerAPI\n",
    "napi = NumerAPI()\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Download the training data and feature metadata\n",
    "# This may take a few minutes\n",
    "\n",
    "train = \"v4.2/train_int8.parquet\"\n",
    "features = \"v4.2/features.json\"\n",
    "\n",
    "\n",
    "napi.download_dataset(train);\n",
    "napi.download_dataset(features);\n",
    "\n",
    "# Load the training data but only the \"small\" subset of features to save time and memory\n",
    "# In practice you will want to use all the features to maximize your model's performance\n",
    "feature_metadata = json.load(open(features)) \n",
    "feature_cols = feature_metadata[\"feature_sets\"][\"small\"]\n",
    "training_data = pd.read_parquet(train, columns= [\"era\"] + feature_cols + [\"target\"]) \n",
    "\n",
    "real_submition = True\n",
    "if real_submition:\n",
    "# For better models, join train and validation data and train on all of it\n",
    "    napi.download_dataset(\"v4.2/validation_int8.parquet\");\n",
    "    validation = pd.read_parquet(\"v4.2/validation_int8.parquet\", columns=[\"era\", \"data_type\"]+feature_cols+[\"target\"])\n",
    "    validation = validation[validation[\"data_type\"] == \"validation\"] # drop rows which don't have targets yet\n",
    "    training_data = pd.concat([training_data, validation])\n",
    "\n",
    "\n",
    "\n",
    "# downsample para cada 4rt era , assim nao trein\n",
    "# training_data = training_data[training_data[\"era\"].isin(training_data[\"era\"].unique()[::4])]\n",
    "\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.groupby(\"era\").size().plot(title=\"Number of Rows per Era\", figsize=(5, 3), xlabel=\"Era\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "o objetivo aqui Ã© criar um modelo para submeter no numer.ai\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM\n",
    "isto faz trees\n",
    "\n",
    "primeiro vou criar um modelo com as opcoes default dadas pelo numerai para ver como se sai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "try:\n",
    "    # load aos modelos de treino se ja existir\n",
    "    model = lgb.Booster(model_file='small_lgbm_tree.model');\n",
    "except lgb.basic.LightGBMError:\n",
    "    # se naoe xistir criar o modelo e gravar\n",
    "    model = lgb.LGBMRegressor(\n",
    "        n_estimators = 2000,\n",
    "        learning_rate = 0.01,\n",
    "        max_depth = 5,\n",
    "        num_leaves = 2**5-1,\n",
    "        colsample_bytree = 0.1\n",
    "    )\n",
    "\n",
    "    #treinar o modelo\n",
    "    model.fit(\n",
    "        training_data[feature_cols],\n",
    "        training_data[\"target\"]\n",
    "    )\n",
    "\n",
    "    # gravar o modelo\n",
    "    model.booster_.save_model(\"small_lgbm_tree.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testar o modelo agora\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definir a funcao de scoring para corr\n",
    "esta Ã© a maneira principal de scoring do numerai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "# Numerai's primary scoring metric\n",
    "def numerai_corr(preds, target):\n",
    "    # rank (keeping ties) then gaussianize predictions to standardize prediction distributions\n",
    "    ranked_preds = (preds.rank(method=\"average\").values - 0.5) / preds.count()\n",
    "    gauss_ranked_preds = stats.norm.ppf(ranked_preds)\n",
    "    # center targets around 0\n",
    "    centered_target = target - target.mean()\n",
    "    # raise both preds and target to the power of 1.5 to accentuate the tails\n",
    "    preds_p15 = np.sign(gauss_ranked_preds) * np.abs(gauss_ranked_preds) ** 1.5\n",
    "    target_p15 = np.sign(centered_target) * np.abs(centered_target) ** 1.5\n",
    "    # finally return the Pearson correlation\n",
    "    return np.corrcoef(preds_p15, target_p15)[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the validation data and prepare for scoring\n",
    "\n",
    "# Download validation data \n",
    "# This will take a few minutes ðŸµ\n",
    "napi.download_dataset(\"v4.2/validation_int8.parquet\");\n",
    "\n",
    "# Load the validation data, filtering for data_type == \"validation\"\n",
    "validation_data = pd.read_parquet(\"v4.2/validation_int8.parquet\", columns=[\"era\", \"data_type\"] + feature_cols + [\"target\"]) \n",
    "validation_data = validation_data[validation_data[\"data_type\"] == \"validation\"]\n",
    "del validation_data[\"data_type\"]\n",
    "\n",
    "# Downsample to every 4th era to reduce memory usage and speedup evaluation (suggested for Colab free tier)\n",
    "# Comment out the line below to use all the data (higher memory usage, slower inference, more accurate evaluation)\n",
    "validation_data = validation_data[validation_data[\"era\"].isin(validation_data[\"era\"].unique()[::4])]\n",
    "\n",
    "# Eras are 1 week apart, but targets look 4 weeks into the future, so we need to \"embargo\" the 4 eras following our last train era to avoid data leakage. \n",
    "last_train_era = int(training_data[\"era\"].unique()[-1])\n",
    "eras_to_embargo = [str(era).zfill(4) for era in [last_train_era + i for i in range(4)]]\n",
    "validation_data = validation_data[~validation_data[\"era\"].isin(eras_to_embargo)]\n",
    "\n",
    "# Generate predictions against the out-of-sample validation features\n",
    "# This will take a few minutes ðŸµ\n",
    "validation_data[\"prediction\"] = model.predict(validation_data[feature_cols])\n",
    "validation_data[[\"era\", \"prediction\", \"target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot da corr/cummulatia / era, mais util porque permite ter nocao de \"quanto ganha\"__qualname__ \n",
    "per_era_corr = validation_data.groupby(\"era\").apply(lambda x: numerai_corr(x[\"prediction\"], x[\"target\"]))\n",
    "per_era_corr.cumsum().plot(kind=\"line\", title=\"Cumulative Validation Correlation\", figsize=(10, 6));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scoring data function\n",
    "def scoring_data(model, validation_data, feature_cols):\n",
    "    # Generate predictions against the out-of-sample validation features\n",
    "    # This will take a few minutes ðŸµ\n",
    "    validation_data[\"prediction\"] = model.predict(validation_data[feature_cols])\n",
    "    validation_data[[\"era\", \"prediction\", \"target\"]]\n",
    "    per_era_corr = validation_data.groupby(\"era\").apply(lambda x: numerai_corr(x[\"prediction\"], x[\"target\"]))\n",
    "\n",
    "    # Compute performance metrics\n",
    "    corr_mean = per_era_corr.mean()\n",
    "    corr_std = per_era_corr.std(ddof=0)\n",
    "    corr_sharpe = corr_mean / corr_std\n",
    "    max_drawdown = (per_era_corr.cumsum().expanding(min_periods=1).max() - per_era_corr.cumsum()).max()\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"mean\": corr_mean,\n",
    "        \"std\": corr_std,\n",
    "        \"sharpe\": corr_sharpe,\n",
    "        \"max_drawdown\": max_drawdown\n",
    "    }, index=[\"Value\"]).T\n",
    "\n",
    "\n",
    "# scoring_data(model, validation_data, feature_cols)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define predict function\n",
    "def predict(live_features: pd.DataFrame) -> pd.DataFrame:\n",
    "    live_predictions = model.predict(live_features[feature_cols])\n",
    "    submission = pd.Series(live_predictions, index=live_features.index)\n",
    "    return submission.to_frame(\"prediction\")\n",
    "\n",
    "# picklar a funcao\n",
    "import cloudpickle\n",
    "\n",
    "p = cloudpickle.dumps(predict)\n",
    "with open(\"small_lgbm_tree.pkl\", \"wb\") as f:\n",
    "    f.write(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "mesma coisa mas com xgboost\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# Initialize the XGBoost regression model tree\n",
    "model = xgb.XGBRegressor(\n",
    "    n_estimators = 2000,\n",
    "    learning_rate = 0.01,\n",
    "    max_depth = 5,\n",
    "    max_leaves = 2**5,\n",
    "    colsample_bytree = 0.1,\n",
    "    #tree_method='gpu_hist',\n",
    "    #gpu_id= 0,\n",
    "    random_state=420\n",
    "    )\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(\n",
    "    training_data[feature_cols],\n",
    "    training_data[\"target\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define predict function\n",
    "def predict(live_features: pd.DataFrame) -> pd.DataFrame:\n",
    "    live_predictions = model.predict(live_features[feature_cols])\n",
    "    submission = pd.Series(live_predictions, index=live_features.index)\n",
    "    return submission.to_frame(\"prediction\")\n",
    "\n",
    "# picklar a funcao\n",
    "import cloudpickle\n",
    "\n",
    "p = cloudpickle.dumps(predict)\n",
    "with open(\"small_lgbm_tree.pkl\", \"wb\") as f:\n",
    "    f.write(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring and stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlacao /era das previsoes com o metamodelo + um plot\n",
    "\n",
    "per_era_corr = validation_data.groupby(\"era\").apply(lambda x: numerai_corr(x[\"prediction\"], x[\"target\"]))\n",
    "per_era_corr.plot(kind=\"bar\", title=\"Validation Correlation\", figsize=(10, 6), xticks=[]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot da corr/cummulatia / era, mais util porque permite ter nocao de \"quanto ganha\"__qualname__ \n",
    "per_era_corr.cumsum().plot(kind=\"line\", title=\"Cumulative Validation Correlation\", figsize=(10, 6));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Load the tournament data\n",
    "# training_data = training_data\n",
    "\n",
    "# Select features and target variable\n",
    "\n",
    "X = training_data[feature_cols]\n",
    "y = training_data[\"target\"]\n",
    "\n",
    "# Initialize and train the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define predict function\n",
    "def predict(live_features: pd.DataFrame) -> pd.DataFrame:\n",
    "    live_predictions = model.predict(live_features[feature_cols])\n",
    "    submission = pd.Series(live_predictions, index=live_features.index)\n",
    "    return submission.to_frame(\"prediction\")\n",
    "\n",
    "# picklar a funcao\n",
    "import cloudpickle\n",
    "\n",
    "p = cloudpickle.dumps(predict)\n",
    "with open(\"small_lin_reg.pkl\", \"wb\") as f:\n",
    "    f.write(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## quad reg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "X = training_data[feature_cols]\n",
    "\n",
    "\n",
    "# Assuming X contains your input data and y contains your output data\n",
    "polynomial_features = PolynomialFeatures(degree=2)\n",
    "X_poly = polynomial_features.fit_transform(X)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_poly, y)\n",
    "\n",
    "# Assuming X_test contains your test data\n",
    "X_test_poly = polynomial_features.transform()\n",
    "predictions = model.predict(X_test_poly)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define predict function\n",
    "def predict(live_features: pd.DataFrame) -> pd.DataFrame:\n",
    "    X_test_poly = polynomial_features.transform(live_features[feature_cols])\n",
    "    live_predictions = model.predict(X_test_poly)\n",
    "    submission = pd.Series(live_predictions, index=live_features.index)\n",
    "    return submission.to_frame(\"prediction\")\n",
    "\n",
    "# picklar a funcao\n",
    "import cloudpickle\n",
    "\n",
    "p = cloudpickle.dumps(predict)\n",
    "with open(\"small_quad_reg.pkl\", \"wb\") as f:\n",
    "    f.write(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Octo Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "numerai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
